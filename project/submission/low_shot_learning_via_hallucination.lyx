#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{icml2018}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Low-Shot-Learning of Diseases in Chest X-Rays via Hallucination
\end_layout

\begin_layout Author
Matan Harel 10%, Uri Avron 50% & Jonathan Somer 40%
\end_layout

\begin_layout Abstract

\series bold
One of the promises
\series default
 of the recent advancements in Artificial Intelligence is the ability to
 facilitate high precision computer aided diagnosis (CAD) systems and make
 such high precision diagnosis affordable and highly available.
 Medical imaging is one area where current deep learning and computer vision
 methods could possibly be applied.
 These methods utilize large sets of labelled images in order to achieve
 high accuracy.
 High quality labelled data is difficult to obtain due to a variety of factors.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Our objective is to answer whether we can use an existing large set of X-Ray
 images of healthy patients as well as a large of set of images of patients
 with some diseases, in order to improve the learning accuracy of new diseases
 for which we have only a few example images to learn from.
 This setting is known as Low-Shot-Learning.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We have implemented, extended and re-evaluated a method for performing low-shot-
learning proposed by Bharath Hariharan and Ross Girshick from Facebook AI
 research, 2016 
\begin_inset Formula $[1]$
\end_inset

.
 During this process we have also attempted to answer some questions that
 remained open while reading their innovative original paper and we  show
 some novel methods for evaluating this low-shot-learning setting.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Low-Shot-Learning
\end_layout

\begin_layout Standard
The ability to learn from
\series bold
 
\series default
very few examples is a hallmark of human visual intelligence.
 
\series bold
Classical Machine Learning approaches fail to generalize[add reference]
\series default
 from few examples so new techniques are required for performing Low-Shot-Learni
ng.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The setting for low-shot-learning is composed of two phases.
 The first is a representation learning phase: the learner tunes its feature
 representation on an available set of base classes that have many training
 instances.
 In the low-shot learning phase, the learner is exposed to a set of novel
 classes with only a few examples per class and must learn a classifier
 over the joint label space of base and novel classes.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We evaluate the new classifier's accuracy over both the base and novel classes
 in order to see that higher accuracy was achieved on the novel classes
 but also that accuracy was not impaired for the base classes.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Low-Shot-Learning is a great challenge particularly in our setting as learning
 in a medical setting carries many unique challenges.
 Each disease category contains great intra-class variation: patients have
 varying anatomies, there are different methods of performing each examination,
 variation might be induced by different equipment and so on.
 Thus, very large labelled datasets are needed in order to capture this
 great variation.
 Obtaining high quality labelled datasets as such is very difficult.
 In addition, even if labelled data is obtained, the physician's diagnosis
 can be incorrect and in many cases is not validated or such validation
 does not get logged.
 In the dataset we chose to work with the researches who gathered the data
 employed a NLP text mining solution to procure labels from physicians written
 reports, a process which 
\series bold
possibly adds further noise[how much exactly + ref.]
\series default
 to the labelling.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Thus, high quality labelled data is difficult to obtain and the ability
 to learn from little data is highly valuable.
\end_layout

\begin_layout Subsection
Our Approach - High Level Glance
\end_layout

\begin_layout Standard
As noted, our approach is primarily based on the paper by Bharath Hariharan
 and Ross Girshick, 2016 
\begin_inset Formula $[1]$
\end_inset

.
 At a very high level, the method is as follows: given
\series bold
 only few
\series default
 samples of a novel class, we train a generator network to generate many
 new 'hallucinated' examples for this same class.
 We then use those new generated examples, as well as those that were given
 to us, in order to train another neural network to perform the classification
 task.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The method for generating new training examples is based on the insight
 that variation within one category might be transferable to another category.
 For instance, a certain variation in anatomy may impact the chest images
 similarly, regardless of the disease.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We  elaborate on the points in which we implemented novel solutions and
 advancements as we dive into the details of the method .
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Section
Our Approach
\end_layout

\begin_layout Standard
The method is composed of 3 parts:
\end_layout

\begin_layout Enumerate
Feature representation learning
\end_layout

\begin_layout Enumerate
Training a generator
\end_layout

\begin_layout Enumerate
Generating data using the generator
\end_layout

\begin_layout Enumerate
Training a classifier using the generated data
\end_layout

\begin_layout Standard
We first introduce the datasets we used and then describe the method in
 detail.
\end_layout

\begin_layout Subsection
Datasets used
\end_layout

\begin_layout Standard
We tested our results on 3 different datasets.
 The first 2 are the well researched MNIST and CIFAR10 datasets.
 The third is a new chest X-ray image dataset.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

In May 2017, the "ChestX-ray8" dataset was presented by a team of researchers
 from the NIH 
\begin_inset Formula $[2]$
\end_inset

.
 In the paper they present the methods used to generate the data.
 A short summary of the data is as follows:
\end_layout

\begin_layout Itemize
108,948 images of 32,717 patients.
\end_layout

\begin_layout Itemize
8 disease labels text-mined from radiological reports.
\end_layout

\begin_layout Itemize
Each image is labeled with 'Normal' or labelled with more of the 8 disease
 labels, or 
\end_layout

\begin_layout Itemize
Labeling: classes are very imbalanced.
 For example: 84K images were tagged 'Normal' and around 1K were tagged
 with 'Cardiomegaly '.
\end_layout

\begin_layout Itemize
Image sizes are 
\begin_inset Formula $1024\times1024$
\end_inset

 pixels.
 These are relatively large images, recall MNIST images are 
\begin_inset Formula $28\times28$
\end_inset

.
 The entire dataset takes around 40GB of space.
\end_layout

\begin_layout Standard
Along with the data, they also provide a benchmark for the task of classifying
 diseases using a deep convolutional neural network (DCNN) they have trained.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Subsection
Feature Representation Learning
\end_layout

\begin_layout Standard
For the X-ray dataset we used a ResNet50 DCNN pre-trained on the very large
 and diverse ImageNet dataset, without the last dense layer, in order to
 generate the features for the images.
 This method was used in the chest x-ray paper in order to train a classifier
 on the data and implementing the same method enabled us to first recreate
 their results and continue from there onto low shot learning.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

For the MNIST and CIFAR10 datasets we trained two different CNNs which achieved
 ~99% and ~90% accuracy on the datasets respectively.
 In the low-shot-learning setting we do not have access to data from the
 novel class during representation learning.
 Thus, we treated each class in turn as the novel class , and trained a
 classifier on the remaining classes.
 We then used this classifier, with the last layer removed, as a feature
 extractor in order to generate features for the novel class as well.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

There is a significant difference between the two methods.
 In our method, representation learning is performed ad hoc for the specific
 setting, intuitively - 
\begin_inset Quotes eld
\end_inset

we learn to represent digits by learning to recognize digits
\begin_inset Quotes erd
\end_inset

.
 Whereas, in the first setting a generic network is used to generate features
 for images from a very specific domain.
 
\series bold
As further research we propose to train a DCNN from scratch on the X-Ray
 dataset and compare the two methods
\series default
.
 We presume that features learnt from data that is close to the domain at
 hand will be better than those obtained by generic models.
\end_layout

\begin_layout Subsection
Learning to Generate New Examples
\end_layout

\begin_layout Standard
We now train a generator 
\begin_inset Formula $G$
\end_inset

 for hallucinating images for novel classes.
 As we have noted, the method for generating new training examples is based
 on the insight that variation within one category might be transferable
 to another category.
 We want to use the knowledge of the intra-class variation of one class
 in order to generate a diverse set of examples for the novel class.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We train 
\begin_inset Formula $G$
\end_inset

 to 
\begin_inset Quotes eld
\end_inset

solve analogies
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Formula $G$
\end_inset

 will receive as input the concatenated feature vectors 
\begin_inset Formula $\langle\phi(b_{1}),\phi(b_{2}),\phi(x)\rangle$
\end_inset

 where 
\begin_inset Formula $b_{1},b_{2}$
\end_inset

 are two samples from the same base class and 
\begin_inset Formula $x$
\end_inset

 is a novel image.
 For this input, 
\begin_inset Formula $G$
\end_inset

 will output a vector who solves the analogy 
\begin_inset Formula $b_{1}:b_{2}\Rightarrow x:\ ?$
\end_inset

 Thus applying to 
\begin_inset Formula $x$
\end_inset

 the 
\begin_inset Formula $b_{1}\rightarrow b_{2}$
\end_inset

 transformation.
 Note that the 
\begin_inset Formula $b_{1}\rightarrow b_{2}$
\end_inset

 transformation stays within class 
\begin_inset Formula $B$
\end_inset

 and the generator should perform on 
\begin_inset Formula $x$
\end_inset

 a transformation that does not result in an element of a different class
 than 
\begin_inset Formula $x$
\end_inset

.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $G$
\end_inset

 will be a 3 layer multi-layer-perceptron.
 The training data for 
\begin_inset Formula $G$
\end_inset

 is generated by creating completed analogies - quadruplets of feature vectors
 from the base classes.
 We start by clustering each of the base classes into 
\begin_inset Formula $k$
\end_inset

 clusters.
 Then for each two classes 
\begin_inset Formula $A,B$
\end_inset

, for each pair of centroids, 
\begin_inset Formula $c_{1}^{A},c_{2}^{A}$
\end_inset

 from class 
\begin_inset Formula $A$
\end_inset

 we find the pair 
\begin_inset Formula $c_{1}^{B},c_{2}^{B}$
\end_inset

 such that the cosine distance between 
\begin_inset Formula $c_{1}^{A}-c_{2}^{A}$
\end_inset

 and 
\begin_inset Formula $c_{1}^{B}-c_{2}^{B}$
\end_inset

 is minimized.
 Concatenating the 4 centroids results in one element in the dataset.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

For each quadruplet 
\begin_inset Formula $\langle c_{1}^{A},c_{2}^{A},c_{1}^{B},c_{2}^{B}\rangle$
\end_inset

, we feed the triplet: 
\begin_inset Formula $\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle$
\end_inset

 into 
\begin_inset Formula $G$
\end_inset

.
 We want 
\begin_inset Formula $G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle)$
\end_inset

 to be as close as possible to 
\begin_inset Formula $c_{2}^{B}$
\end_inset

 and also remain within the class 
\begin_inset Formula $B$
\end_inset

.
 In order to do so we minimize the loss function: 
\begin_inset Formula 
\[
\lambda MSE(G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle),c_{2}^{B})+(1-\lambda)L_{cls_{BASE}}(G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle),B)
\]

\end_inset

 Where we have 
\begin_inset Formula $MSE$
\end_inset

 the mean square error between the generator's output and the true target
 and 
\begin_inset Formula $L_{cls_{BASE}}$
\end_inset

 the log-loss of the classifier w.r.t the true class of 
\begin_inset Formula $c_{2}^{B}$
\end_inset

.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Subsubsection
Evaluating the Generalization Capacity of the Generator
\end_layout

\begin_layout Standard
We train the generator on one set of classes, but then use it to generate
 data for new classes.
 Does a generator trained to perform transformations that stay within one
 set of classes respect this constraint on a novel class it has never seen
 before? 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The authors of the paper in which this concept is presented do not address
 this issue directly and we wished to test this explicitly.
 In order to do so we trained another - 
\begin_inset Quotes eld
\end_inset

all knowing
\begin_inset Quotes erd
\end_inset

 classifier.
 This classifier was trained on the feature vectors of all classes including
 many examples of the novel class.
 We assume that if the generator generalizes well, then the transformations
 it performs on an example from a novel class it has never seen before the
 output will also remain within the same class.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The results show that this is indeed the case:
\end_layout

\begin_layout Quote
++BENCHMARK PLOT++
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Enumerate
describe the pipeline experiment.
 pit falls etc.
 the most we can squeeze out of the generator
\end_layout

\begin_layout Enumerate
figures and results.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Low-shot Visual Recognition by Shrinking and Hallucinating Features"
key "key-1"

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks "
key "key-2"

\end_inset


\end_layout

\end_body
\end_document
