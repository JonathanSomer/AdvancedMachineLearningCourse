#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\date{}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{icml2018}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 0.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[
\end_layout

\begin_layout Plain Layout


\backslash
icmltitle{Low-Shot-Learning of Diseases in Chest X-Rays via Hallucination}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
begin{icmlauthorlist} 
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Uri Avron}{tau}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Jonathan Somer}{tau}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Matan Harel}{tau}
\end_layout

\begin_layout Plain Layout


\backslash
end{icmlauthorlist}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
icmlaffiliation{tau}{Tel Aviv University, Tel Aviv, Israel} 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
icmlcorrespondingauthor{Uri Avron}{uriavron@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlcorrespondingauthor{Jonathan Somer}{somer@mail.tau.ac.il}
\backslash
icmlcorrespondingauthor{Matan Harel}{matan.harel.mh@gmail.com}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
One of the promises of the recent advancements in Artificial Intelligence
 is the ability to facilitate high precision computer aided diagnosis (CAD)
 systems and make such high precision diagnosis affordable and highly available.
 Current methods utilize large labelled datasets in order to achieve high
 accuracy, but such labelled data is difficult to obtain.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Our objective is to answer whether we can use an existing large set of X-Ray
 images of healthy patients as well as a large of set of images of patients
 with some diseases, in order to improve the learning accuracy of new diseases
 for which we have only a few example images to learn from.
 This setting is known as Low-Shot-Learning.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We have implemented and extended a method for performing low-shot-learning
 proposed in 
\begin_inset Formula $[1]$
\end_inset

 and show some novel methods for evaluating this low-shot-learning setting.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Low-Shot-Learning
\end_layout

\begin_layout Standard
The setting for low-shot-learning is composed of two phases.
 The first is a representation learning phase: the learner tunes its feature
 representation on an available set of base classes that have many training
 instances.
 In the low-shot learning phase, the learner is exposed to a set of novel
 classes with only a few examples per class and must learn a classifier
 over the joint label space of base and novel classes.
 We evaluate the new classifier's accuracy over both the base and novel
 classes in order to see that higher accuracy was achieved on the novel
 classes but also that accuracy was not impaired for the base classes.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Low-Shot-Learning is a great challenge particularly in the medical setting.
 Patients have varying anatomies, there are different methods of performing
 each examination, variation might be induced by different equipment and
 so on.
 Thus, very large labelled datasets are needed in order to capture this
 great variation.
 Obtaining high quality labelled datasets as such is very difficult.
 In addition, even if labelled data is obtained, the physician's diagnosis
 can be incorrect and in many cases is not validated or such validation
 does not get logged.
 The researchers who gathered the chest X-ray dataset 
\begin_inset Formula $[2]$
\end_inset

 employed a NLP text mining solution to procure labels from physicians written
 reports, a process which
\series bold
 
\series default
adds further noise to the labelling.
 Thus, high quality labelled data is difficult to obtain and the ability
 to learn from little data is highly valuable.
\end_layout

\begin_layout Subsection
Our Approach - High Level Glance
\end_layout

\begin_layout Standard
Given
\series bold
 
\series default
only few samples of a novel class, we use the abundant data for the base
 classes to train a generator network which can generate many new 'hallucinated'
 examples for the novel class.
 We then use those generated examples, as well as those that were given
 to us, in order to train another neural network to perform the classification
 task.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The method for generating new training examples is based on the insight
 that variation within one category might be transferable to another category.
 For instance, a certain variation in anatomy may impact the chest images
 similarly, regardless of the disease.
\end_layout

\begin_layout Section
Our Approach
\end_layout

\begin_layout Subsection
Datasets used
\end_layout

\begin_layout Standard
We tested our results on 3 different datasets.
 The first 2 are the well researched MNIST and CIFAR10 datasets.
 The third is a new chest X-ray image dataset.
 In May 2017, the "ChestX-ray8" dataset, which contains over 
\begin_inset Formula $100K$
\end_inset

 
\begin_inset Formula $1024\times1024$
\end_inset

 resolution images, was presented by a team of researchers from the NIH
 
\begin_inset Formula $[2]$
\end_inset

.
 They presented the methods used to generate the data along with a benchmark
 for the task of classifying diseases using a deep convolutional neural
 network (DCNN) they have trained.
 A succinct summary of this dataset is provided in appendix 
\begin_inset Formula $5.1$
\end_inset

.
\end_layout

\begin_layout Subsection
Phase 1 - Feature Representation Learning
\end_layout

\begin_layout Standard
The method we used is composed of 4 phases we describe now.
 For the X-ray dataset we used a ResNet50 DCNN pre-trained on the very large
 and diverse ImageNet dataset, without the last dense layer, in order to
 generate the features for the images.
 This method was used in 
\begin_inset Formula $[2]$
\end_inset

 in order to train a classifier on the data and implementing the same method
 enabled us to first recreate their results and continue from there onto
 low shot learning.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

For the MNIST and CIFAR10 datasets we trained two different CNNs which achieved
 ~99% and ~90% accuracy on the datasets respectively.
 In the low-shot-learning setting we do not have access to data from the
 novel class during representation learning.
 Thus, we treated each class in turn as the novel class , and trained a
 classifier on the remaining classes.
 We then used this classifier, with the last layer removed, as a feature
 extractor in order to generate features for the novel class as well.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Note that there is a significant difference between the two methods.
 In our method, representation learning is performed ad hoc for the specific
 setting, intuitively - 
\begin_inset Quotes eld
\end_inset

we learn to represent digits by learning to recognize digits
\begin_inset Quotes erd
\end_inset

.
 Whereas, in the first setting a generic network is used to generate features
 for images from a very specific domain.
 As further research we propose to train a DCNN from scratch on the X-Ray
 dataset and compare the two methods.
 We presume that features learnt from data that is close to the domain at
 hand will be better than those obtained by generic models.
\end_layout

\begin_layout Subsection
Phase 2 - Learning to Generate New Examples
\end_layout

\begin_layout Standard
We now train a generator 
\begin_inset Formula $G$
\end_inset

 for hallucinating images for novel classes.
 We train 
\begin_inset Formula $G$
\end_inset

 to 
\begin_inset Quotes eld
\end_inset

solve analogies
\begin_inset Quotes erd
\end_inset

: 
\begin_inset Formula $G$
\end_inset

 will receive as input the concatenated feature vectors 
\begin_inset Formula $\langle\phi(b_{1}),\phi(b_{2}),\phi(x)\rangle$
\end_inset

 where 
\begin_inset Formula $b_{1},b_{2}$
\end_inset

 are two samples from the same base class and 
\begin_inset Formula $x$
\end_inset

 is a novel image.
 For this input, 
\begin_inset Formula $G$
\end_inset

 will output a vector who solves the analogy 
\begin_inset Formula $b_{1}:b_{2}\Rightarrow x:\ ?$
\end_inset

 Thus applying to 
\begin_inset Formula $x$
\end_inset

 the 
\begin_inset Formula $b_{1}\rightarrow b_{2}$
\end_inset

 transformation.
 Note that the 
\begin_inset Formula $b_{1}\rightarrow b_{2}$
\end_inset

 transformation stays within class 
\begin_inset Formula $B$
\end_inset

 and the generator should perform on 
\begin_inset Formula $x$
\end_inset

 a transformation that does not result in an element of a different class
 than 
\begin_inset Formula $x$
\end_inset

 (see part 
\begin_inset Formula $3.3$
\end_inset

 for further evaluation of the generator's performance).
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $G$
\end_inset

 will be a 3 layer MLP.
 The training data for 
\begin_inset Formula $G$
\end_inset

 is generated by creating completed analogies - quadruplets of feature vectors
 from the base classes.
 We start by clustering each of the base classes into 
\begin_inset Formula $k$
\end_inset

 clusters (we tested different 
\begin_inset Formula $k$
\end_inset

 ranging from 5 to 50).
 Then for each two classes 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

, for each pair of centroids, 
\begin_inset Formula $c_{1}^{A},c_{2}^{A}$
\end_inset

 from class 
\begin_inset Formula $A$
\end_inset

 we find the pair 
\begin_inset Formula $c_{1}^{B},c_{2}^{B}$
\end_inset

 such that the cosine distance between 
\begin_inset Formula $c_{1}^{A}-c_{2}^{A}$
\end_inset

 and 
\begin_inset Formula $c_{1}^{B}-c_{2}^{B}$
\end_inset

 is minimized.
 Concatenating these 4 centroids results in one element in the dataset.
 For each quadruplet 
\begin_inset Formula $\langle c_{1}^{A},c_{2}^{A},c_{1}^{B},c_{2}^{B}\rangle$
\end_inset

, we feed the triplet: 
\begin_inset Formula $\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle$
\end_inset

 into 
\begin_inset Formula $G$
\end_inset

.
 We want 
\begin_inset Formula $G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle)$
\end_inset

 to be as close as possible to 
\begin_inset Formula $c_{2}^{B}$
\end_inset

 and also remain within the class 
\begin_inset Formula $B$
\end_inset

.
 In order to do so we minimize the loss function: 
\begin_inset Formula 
\begin{align*}
 & \lambda\cdot MSE(G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle),c_{2}^{B})\\
+ & (1-\lambda)\cdot L_{cls_{BASE}}(G(\langle c_{1}^{A},c_{2}^{A},c_{1}^{B}\rangle),B)
\end{align*}

\end_inset

 Where we have 
\begin_inset Formula $MSE$
\end_inset

 the mean square error between the generator's output and the true target
 and 
\begin_inset Formula $L_{cls_{BASE}}$
\end_inset

 the log-loss of the classifier w.r.t the true class of 
\begin_inset Formula $c_{2}^{B}$
\end_inset

.
\end_layout

\begin_layout Subsection
Phase 3 - Generating New Examples for a Novel Class
\end_layout

\begin_layout Standard
Assume we have received 
\begin_inset Formula $n$
\end_inset

 examples of some novel category.
 Generating a new example is done by sampling one of these examples - 
\begin_inset Formula $\phi(x)$
\end_inset

, choosing a base class 
\begin_inset Formula $A$
\end_inset

 , and from it two centroids - 
\begin_inset Formula $c_{1}^{A},c_{2}^{A}$
\end_inset

.
 We then apply the generator to this triplet; 
\begin_inset Formula $G(\langle c_{1}^{A},c_{2}^{A},\phi(x)\rangle)$
\end_inset

 is the generated example.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

In 
\begin_inset Formula $[1]$
\end_inset

, the choice of category and centroids was performed uniformly at random.
 Recall that our goal is to compensate for the lack of intra-class variation
 contained within the samples of the novel class.
 Thus, we would like to generate this variation utilizing information about
 other classes.
 We propose a novel way of performing this selection which increases the
 likelihood that the variation within the chosen base class 
\begin_inset Formula $A$
\end_inset

 will be transferable to the novel class.
 Recall that during the feature representation phase we have trained a classifie
r over the base classes.
 We suggest that by applying this classifier on the samples of the novel
 class, we will gain some insight into which classes might possibly carry
 greater transferability to the novel class.
 For instance, applying the CIFAR classifier to predict the class of a 
\begin_inset Quotes eld
\end_inset

cat
\begin_inset Quotes erd
\end_inset

 example results in 
\begin_inset Quotes eld
\end_inset

dog
\begin_inset Quotes erd
\end_inset

 in many cases.
 It is very likely that the variation in the dog class is much more relevant
 to the cat class than for instance the variation in the airplane class,
 as dogs and cats should appear in similar conditions whereas airplanes
 and cats should not.
 
\end_layout

\begin_layout Subsection
Phase 4 - Training a Classifier on Generated Data
\end_layout

\begin_layout Standard
The main consideration here is that the low-shot setting creates a significant
 class imbalance.
 In order to fix this we follow 
\begin_inset Formula $[1]$
\end_inset

 and sample the training data uniformly over classes and uniformly within
 classes.
 
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Subsection
Evaluating Clustering over Feature Space
\end_layout

\begin_layout Standard
One of the assumptions behind this model is that clustering over feature
 space results in capturing the variation in the data.
 We used the K-Means algorithm in order to cluster the data.
 Recall that K-Means produces centroids that are not necessarily in the
 original set of points.
 So in order to test if these centroids capture a variation in the data
 we first found for each centroid the 4 features closest to it.
 We then plotted the 4 images whose feature representations are these 4
 examples.
 Figures 1 and 6 validate the assumption that the clustering grasps the
 intra class variation.
 Note that the total number of clusters was 30 and we randomly chose 4 to
 display.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clusters in Feature Space Capture Intra-class Variation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="top">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_cats_1.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_cats_2.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_cats_4.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_cats_3.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Connection Between Number of Clusters Used in Training and Test Accuracy
 Achieved
\end_layout

\begin_layout Standard
Our generator is trained to perform transformations which move 
\begin_inset Quotes eld
\end_inset

one centroid to another
\begin_inset Quotes erd
\end_inset

.
 There is an underlying assumption here that we must have data from different
 clusters in order to capture the variation and perform well on the test
 data.
 We wanted to test this assumption before testing the use of generated data.
 We first plot for each dataset the accuracy achieved by the number of unique
 samples used for training.
 Figure 5 shows that the growth is logarithmic in the number of samples.
 We now fix the sample size and test how the number of clusters from which
 the training data is taken affects the accuracy.
 We chose to use 256 samples as we have seen that the growth in accuracy
 diminishes from that point forward (Figure 5).
 Figure 2 shows that taking data from more centroids does indeed increase
 the model's capacity to generalize on the test set.
 All results are averaged over 5 runs.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy Achieved by Number of Clusters Data is Sampled From - CIFAR (Fixed
 Number of Unique Samples)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/acc_by_num_clusters.png
	display false
	width 6cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Evaluating the Performance of the Generator
\end_layout

\begin_layout Subsubsection
Task 1: Transformations Generate new Instances of the Same Class 
\end_layout

\begin_layout Standard
In essence, the generator must perform well on two distinct tasks.
 The first: given a sample from a novel class the generator must generate
 samples which are from the same class.
 This is non-trivial since it has never trained on data from this class.
 There is an underlying assumption that training the generator to perform
 transformations which leave examples from the base classes in the same
 class will result in the generator performing similarly on the novel class.
 As the generated examples do not correspond with real images, we devised
 the following test to check if they are from the correct class: we first
 train a classifier on all examples of the base and novel classes, this
 classifier serves as our 
\begin_inset Quotes eld
\end_inset

oracle
\begin_inset Quotes erd
\end_inset

.
 We expect that this classifier will recognize the generated images from
 samples of the novel class as instances of the novel class, with high probabili
ty.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

At this point we also provide a comparison between the standard method of
 generating data - choosing a category at random, and our 
\begin_inset Quotes eld
\end_inset

smart selection
\begin_inset Quotes erd
\end_inset

 of a category.
 We show that this new method provides some improvement over the standard
 method of generation.
 Figure 3 plots this comparison, our method in orange.
 The 
\begin_inset Formula $X$
\end_inset

 axis corresponds with the class removed while training the generator -
 this is the novel class.
 Note the rightmost column which depicts the average accuracy over all categorie
s.
 We also provide the parameters we used: The number of neurons used for
 the generators hidden layers: 
\begin_inset Formula $256$
\end_inset

, and the parameter 
\begin_inset Formula $\lambda=0.95$
\end_inset

 used in the loss function.
 These were discovered via cross validation.
 We fix these parameters from here on.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy Acheived on Generated Data + Comparison of Methods - MNIST
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/accuracy_on_generated_MNIST.png
	width 5cm

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Task 2: Samples Generated have Significant Variation
\end_layout

\begin_layout Standard
Notice that this first task could be achieved trivially by returning the
 sample received as it is.
 If the 
\begin_inset Quotes eld
\end_inset

oracle
\begin_inset Quotes erd
\end_inset

 classifier recognizes this sample to be from its correct class, this trivial
 method will achieve 100% accuracy in the previous test we provided.
 This will not provide for any additional capacity for generalization.
 
\begin_inset Newline newline
\end_inset

Thus, we must also evaluate the generators ability to induce variation in
 the generated samples.
 We do this by first clustering all of the novel class's data (not only
 the samples given to us).
 We now perform the following test, using the MNIST dataset: from each of
 the clusters we draw a single sample, we then generate 
\begin_inset Formula $256$
\end_inset

 new samples from it using our generator.
 We then count the number of clusters these generated examples came from.
 As a sanity check we also re-evaluated the accuracy achieved by the "oracle"
 classifier and ensured that it was sufficiently high as before.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

The average number of unique clusters the data came from was 
\begin_inset Formula $5.84$
\end_inset

 for the standard method of random category selection and 
\begin_inset Formula $5.62$
\end_inset

 for our 
\begin_inset Quotes eld
\end_inset

smart
\begin_inset Quotes erd
\end_inset

 category selection.
 We conclude that the generator does indeed supply variance in the generated
 data.
 
\end_layout

\begin_layout Subsubsection
Comparing the Generator's Performance on the Different Datasets
\end_layout

\begin_layout Standard
At this point, after testing different sets of hyper-parameters, we could
 not arrive at significant accuracy on generated samples from the X-ray
 dataset.
 Furthermore, we have noticed that the generator trained on the X-ray dataset
 generates the same point no matter what input it receives.
 We attribute this to the generator's lack of ability to discover any underlying
 patterns in this noisy data, at which point it 
\begin_inset Quotes eld
\end_inset

settled
\begin_inset Quotes erd
\end_inset

 for some mid-point which minimizes expected loss.
 Note that our architecture was able to achieve high accuracies with both
 the MNIST dataset and the CIFAR10 dataset.
 
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

We attribute this difference to the noisy labels of the X-ray dataset and
 the size and complexity of its images.
 Testing this, we attempted to train a DCNN on this data but could not achieve
 an accuracy higher than 25%.
 We conclude that achieving a sufficient accuracy on the base categories
 is a pre-requisite to performing low-shot learning on a dataset as training
 the generator using the loss from a weak classifier simply adds noise to
 the training.
\end_layout

\begin_layout Subsection
Training A Classifier On Generated Data
\end_layout

\begin_layout Standard
We first perform the following experiment from 
\begin_inset Formula $[1]$
\end_inset

: for each 
\begin_inset Formula $n$
\end_inset

 in 
\begin_inset Formula $\{1,2,5,10,20\}$
\end_inset

 we generate 
\begin_inset Formula $20-n$
\end_inset

 new samples and plot the accuracy achieved.
 We were able to recreate the results achieved in 
\begin_inset Formula $[1]$
\end_inset

 - improvement for small 
\begin_inset Formula $n$
\end_inset

 (in 
\begin_inset Formula $[1]$
\end_inset

 only for 
\begin_inset Formula $n=1,2$
\end_inset

).
 Figure 4 shows the results of this experiment for a single class from CIFAR10.
\end_layout

\begin_layout Standard
Note that in the paper they have used top-5 accuracy so the exact accuracy
 we achieved is in a different order of magnitude.
 Figure 7 shows the results from 
\begin_inset Formula $[1]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy Over number of Original Samples Given
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/WhatsApp Image 2018-09-04 at 21.40.55.jpeg
	width 6cm

\end_inset


\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
We decided to implement yet another test.
 What we believe to be the most significant for a production setting is
 achieving the maximum accuracy possible for a given constant number of
 examples given.
 We plotted this exactly, using a constant number of 
\begin_inset Formula $5$
\end_inset

 examples.
 Figure 8 shows the results.
 We believe that the generated data is noisy and from some amount of generated
 samples, the proportion of true samples to generated samples becomes such
 that accuracy is impaired.
 
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
We started this project with the goal of achieving high precision low-shot
 learning on a large-scale, high resolution medical dataset.
 As we progressed we have found that our focus moved to coming up with novel
 ways to evaluate and visualize the method proposed for low-shot-learning.
 Given the results we have achieved we conclude that many of the underlying
 assumptions for this model are indeed correct.
 We suggest considering alternative architectures for the generator as it
 fails to generate examples that can be used to achieve high accuracies.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

One of the most interesting insights we have arrived at is that there exists
 a significant gap between data which is recognizable as part of some class
 and data that can be used to learn this class from.
 We have seen that the generated examples can be recognized as elements
 of a class with great consistency, but a new classifier, trained on these
 instances does not perform well.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "low_show_learning"
options "plain"

\end_inset


\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Subsection
Accuracy as Function of the Number of Samples used For Training
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy Achieved by Number of Unique Samples Trained On - CIFAR
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/mnist_acc_by_num_samples.png
	display false
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Chest X-Ray8 Dataset Summary
\end_layout

\begin_layout Itemize
108,948 images of 32,717 patients.
\end_layout

\begin_layout Itemize
8 disease labels text-mined from radiological reports.
\end_layout

\begin_layout Itemize
Each image is labeled with 'Normal' or labelled with more of the 8 disease
 labels, or 
\end_layout

\begin_layout Itemize
Labeling: classes are very imbalanced.
 For example: 84K images were tagged 'Normal' and around 1K were tagged
 with 'Cardiomegaly '.
\end_layout

\begin_layout Itemize
Image sizes are 
\begin_inset Formula $1024\times1024$
\end_inset

 pixels.
 These are relatively large images, recall CIFAR images are 
\begin_inset Formula $32\times32$
\end_inset

.
 The entire dataset takes around 40GB of space.
\end_layout

\begin_layout Subsection
Resources
\end_layout

\begin_layout Standard
We utilized 3 google cloud instances, equipped with Nvidia Tesla V100 GPUs.
 This setup enabled us to parallelize cross validation and brought down
 computation time from days to hours for some tasks.
 
\end_layout

\begin_layout Subsection
Clustering Over Feature Space - Results for MNIST
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Clustering over Feature Space Captures Intra-class Variation - Digits
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
Cluster1: 
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_1_1.png
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
Cluster2: 
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_1s_2.png
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
Cluster3: 
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_1s_3.png
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
Cluster4: 
\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/clustered_1s_4.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Results From the Original Paper
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy for Novel Class by Number of Samples Given
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/table.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accuracy by Number of Examples Generated
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Accuracy for Novel Class by Number of Samples Generated
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/generated_test_1.png
	width 8cm

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Code For the Project
\end_layout

\begin_layout Standard
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/JonathanSomer/AdvancedMachineLearningCourse/tree/master/projec
t/code
\end_layout

\end_inset


\end_layout

\end_body
\end_document
